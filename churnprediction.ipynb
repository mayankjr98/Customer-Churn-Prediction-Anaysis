{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a9ebf7f",
      "metadata": {
        "id": "3a9ebf7f"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data.\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mayankjr98/Customer-Churn-Prediction-Anaysis/refs/heads/main/CustomerChurnTrainingData.csv')\n",
        "\n",
        "# Display the first 10 rows of the dataframe.\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "G9GcKk8FkaGF"
      },
      "id": "G9GcKk8FkaGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d77f3a98",
      "metadata": {
        "id": "d77f3a98",
        "outputId": "04383320-3e69-4261-a0d8-a84502d8fe48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Check the shape of the dataframe.\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ea0cb3",
      "metadata": {
        "id": "70ea0cb3"
      },
      "outputs": [],
      "source": [
        "# Check the data types of the features.\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "468d695b",
      "metadata": {
        "id": "468d695b"
      },
      "outputs": [],
      "source": [
        "# Summary statistics.\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0c0b61",
      "metadata": {
        "id": "8e0c0b61"
      },
      "source": [
        "#### 2. Univariate Analysis:\n",
        "\n",
        "Moving forward to the univariate analysis, we will generate **histograms for each numeric feature in the dataset** to help us understand their distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d42187",
      "metadata": {
        "id": "83d42187"
      },
      "outputs": [],
      "source": [
        "# Plot histograms of each numeric feature for univariate analysis.\n",
        "df.hist(figsize=(10, 10), bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415e2a0f",
      "metadata": {
        "id": "415e2a0f"
      },
      "source": [
        "#### 3. Bivariate Analysis:\n",
        "\n",
        "For the Bivariate Analysis we will generate a correlation heatmap, allowing us to understand the relationships between different features of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217bd63f",
      "metadata": {
        "id": "217bd63f"
      },
      "outputs": [],
      "source": [
        "# Plot correlation matrix to understand the relationship between features.\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26600374",
      "metadata": {
        "id": "26600374"
      },
      "source": [
        "#### 4. Missing Value Treatment:\n",
        "\n",
        "For the latest part of the Data Exploration section, we aim to identify the number of missing values in each feature. Depending on the results, we can determine how to handle the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280e96a9",
      "metadata": {
        "id": "280e96a9"
      },
      "outputs": [],
      "source": [
        "# Check for missing values.\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a236c6af",
      "metadata": {
        "id": "a236c6af"
      },
      "source": [
        "#### Feature Engineering\n",
        "\n",
        "For the Feauture Engineering we will create the ratio of `sum_collect_points` to `sum_redeem_points` as an indicator of customer's tendency to save points.\n",
        "\n",
        "Then we will interpret the results of this ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbbba4f",
      "metadata": {
        "id": "2bbbba4f"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "df['collect_to_redeem_ratio'] = df['sum_collect_points'] / df['sum_redeem_points']\n",
        "df['collect_to_redeem_ratio'].replace(np.inf, 0, inplace=True) # Replace any infinity values caused by division by zero.\n",
        "df['collect_to_redeem_ratio'].fillna(0, inplace=True) # Replace any NaN values resulting from nulls in the data.\n",
        "\n",
        "# Corrected Python Code\n",
        "df['collect_to_redeem_ratio'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a89b4376",
      "metadata": {
        "id": "a89b4376"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['collect_to_redeem_ratio'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4424ea",
      "metadata": {
        "id": "4e4424ea"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Divide the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('state', axis=1), df['state'], test_size=0.2, random_state=556555)\n",
        "\n",
        "# Initialize the Logistic Regression model.\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Fit the model to the training data.\n",
        "lr_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd107dd",
      "metadata": {
        "scrolled": false,
        "id": "ffd107dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Evaluate the model on the test set (Accuracy).\n",
        "y_test_pred = lr_model.predict(X_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
        "\n",
        "# Print a classification report.\n",
        "print(\"\\nClassification Report:\\n\\n\", classification_report(y_test, y_test_pred), \"\\n\")\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "\n",
        "# Confusion matrix.\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# Convert confusion matrix to dataframe for better visualization.\n",
        "cm_df = pd.DataFrame(cm, index = ['Active (Actual)', 'Lapsed (Actual)'],\n",
        "                     columns = ['Active (Predicted)', 'Lapsed (Predicted)'])\n",
        "\n",
        "# Display the confusion matrix.\n",
        "display(cm_df)\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the confusion matrix.\n",
        "confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap='Reds')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# ROC curve.\n",
        "# Calculate the probabilities of getting the positive class.\n",
        "y_scores = lr_model.predict_proba(X_test)[:,1]\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "\n",
        "# Plot the ROC curve.\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Misclassification Rate for LR.\n",
        "misclassification_rate_lr = (cm[0][1] + cm[1][0]) / cm.sum()\n",
        "print(\"\\nLogistic Regression Model\")\n",
        "print(\"Misclassification rate: \", misclassification_rate_lr)\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Feature Importance.\n",
        "# Print the feature importance.\n",
        "importance = lr_model.coef_[0]\n",
        "feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance': importance})\n",
        "feature_importance.sort_values(by='importance', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02143b5",
      "metadata": {
        "scrolled": false,
        "id": "a02143b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the decision tree classifier.\n",
        "dt_model = DecisionTreeClassifier(random_state=556555)\n",
        "\n",
        "# Fit the model.\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions.\n",
        "y_dt_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluation.\n",
        "print(\"\\nDecision Tree Model\")\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_dt_pred), \"\\n\")\n",
        "print(classification_report(y_test, y_dt_pred))\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the confusion matrix.\n",
        "confusion_mat_dt = confusion_matrix(y_test, y_dt_pred)\n",
        "sns.heatmap(confusion_mat_dt, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix for Decision Tree')\n",
        "plt.show()\n",
        "\n",
        "# ROC curve and AUC.\n",
        "y_scores_dt = dt_model.predict_proba(X_test)[:,1]\n",
        "roc_auc_dt = roc_auc_score(y_test, y_scores_dt)\n",
        "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_scores_dt)\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_dt, tpr_dt, label='Decision Tree (area = %0.2f)' % roc_auc_dt)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Misclassification rate for Decision Tree Model.\n",
        "misclassification_rate_dt = (confusion_mat_dt[0][1] + confusion_mat_dt[1][0]) / confusion_mat_dt.sum()\n",
        "print(\"\\nDecision Tree Model\")\n",
        "print(\"Misclassification rate: \", misclassification_rate_dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309bc853",
      "metadata": {
        "scrolled": false,
        "id": "309bc853"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the random forest classifier.\n",
        "rf_model = RandomForestClassifier(random_state=556555)\n",
        "\n",
        "# Fit the model.\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions.\n",
        "y_rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation.\n",
        "print(\"\\nRandom Forest Model\")\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_rf_pred), \"\\n\")\n",
        "print(classification_report(y_test, y_rf_pred))\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the confusion matrix.\n",
        "confusion_mat_rf = confusion_matrix(y_test, y_rf_pred)\n",
        "sns.heatmap(confusion_mat_rf, annot=True, fmt=\"d\", cmap='Greens')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix for Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# ROC curve and AUC.\n",
        "y_scores_rf = rf_model.predict_proba(X_test)[:,1]\n",
        "roc_auc_rf = roc_auc_score(y_test, y_scores_rf)\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_scores_rf)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_rf, tpr_rf, label='Random Forest (area = %0.2f)' % roc_auc_rf)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Misclassification rate for Random Forest Model.\n",
        "misclassification_rate_rf = (confusion_mat_rf[0][1] + confusion_mat_rf[1][0]) / confusion_mat_rf.sum()\n",
        "print(\"\\nRandom Forest Model\")\n",
        "print(\"Misclassification rate: \", misclassification_rate_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a590f7ed",
      "metadata": {
        "id": "a590f7ed"
      },
      "source": [
        "---\n",
        "### Model Evaluation and Optimization\n",
        "---\n",
        "\n",
        "After we have built our initial Logistic Regression model, it is crucial to evaluate its performance and optimize it. We'll use cross-validation for a robust estimate of the model's performance and hyperparameter tuning for model optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba004c5",
      "metadata": {
        "id": "7ba004c5"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Create a pipeline that scales the data and then runs logistic regression\n",
        "lr_model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute cross-validation score.\n",
        "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores: \", cv_scores)\n",
        "print(\"Mean cross-validation score: \", np.mean(cv_scores))\n",
        "\n",
        "# Define the parameter grid with 'logisticregression' as prefix for the 'C' parameter\n",
        "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(lr_model, param_grid, cv=5)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters: \", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7928fb55",
      "metadata": {
        "scrolled": false,
        "id": "7928fb55"
      },
      "outputs": [],
      "source": [
        "# Retrain the model using the best parameters\n",
        "best_lr_model = make_pipeline(StandardScaler(), LogisticRegression(C=0.001, max_iter=1000))\n",
        "best_lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set (Accuracy).\n",
        "y_test_pred = best_lr_model.predict(X_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
        "\n",
        "# Print a classification report.\n",
        "print(\"\\nClassification Report:\\n\\n\", classification_report(y_test, y_test_pred), \"\\n\")\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "\n",
        "# Confusion matrix.\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# Convert confusion matrix to dataframe for better visualization.\n",
        "cm_df = pd.DataFrame(cm, index = ['Active (Actual)', 'Lapsed (Actual)'],\n",
        "                     columns = ['Active (Predicted)', 'Lapsed (Predicted)'])\n",
        "\n",
        "# Display the confusion matrix.\n",
        "display(cm_df)\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the confusion matrix.\n",
        "confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap='Purples')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# ROC curve and AUC.\n",
        "# Calculate the probabilities of getting the positive class.\n",
        "y_scores = best_lr_model.predict_proba(X_test)[:,1]\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "\n",
        "# Plot the ROC curve.\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Misclassification Rate for LR.\n",
        "misclassification_rate_lr = (cm[0][1] + cm[1][0]) / cm.sum()\n",
        "print(\"\\nLogistic Regression Model\")\n",
        "print(\"Misclassification rate: \", misclassification_rate_lr)\n",
        "\n",
        "# Print an empty line for clarity purposes.\n",
        "print(\"\\n\")\n",
        "\n",
        "# Feature Importance.\n",
        "# Get the feature importance.\n",
        "importance = best_lr_model.named_steps['logisticregression'].coef_[0]\n",
        "feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance': importance})\n",
        "feature_importance.sort_values(by='importance', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c62ee1",
      "metadata": {
        "scrolled": false,
        "id": "13c62ee1"
      },
      "outputs": [],
      "source": [
        "# Initialize the random forest classifier.\n",
        "rf_model = RandomForestClassifier(random_state=556555)\n",
        "\n",
        "# Fit the model.\n",
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d5e15826",
      "metadata": {
        "id": "d5e15826"
      },
      "outputs": [],
      "source": [
        "# Make predictions.\n",
        "y_rf_pred = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e288c78",
      "metadata": {
        "id": "5e288c78"
      },
      "outputs": [],
      "source": [
        "# Evaluation.\n",
        "print(\"\\nRandom Forest Model\")\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_rf_pred), \"\\n\")\n",
        "print(classification_report(y_test, y_rf_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f4919a",
      "metadata": {
        "id": "21f4919a"
      },
      "outputs": [],
      "source": [
        "# Print the confusion matrix.\n",
        "confusion_mat_rf = confusion_matrix(y_test, y_rf_pred)\n",
        "sns.heatmap(confusion_mat_rf, annot=True, fmt=\"d\", cmap='Greens')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix for Random Forest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82e11c8",
      "metadata": {
        "id": "f82e11c8"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC.\n",
        "y_scores_rf = rf_model.predict_proba(X_test)[:,1]\n",
        "roc_auc_rf = roc_auc_score(y_test, y_scores_rf)\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_scores_rf)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_rf, tpr_rf, label='Random Forest (area = %0.2f)' % roc_auc_rf)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcbe16e",
      "metadata": {
        "id": "5bcbe16e"
      },
      "outputs": [],
      "source": [
        "# Misclassification rate for Random Forest Model.\n",
        "misclassification_rate_rf = (confusion_mat_rf[0][1] + confusion_mat_rf[1][0]) / confusion_mat_rf.sum()\n",
        "print(\"\\nRandom Forest Model\")\n",
        "print(\"Misclassification rate: \", misclassification_rate_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d66005b",
      "metadata": {
        "id": "2d66005b"
      },
      "outputs": [],
      "source": [
        "# Get feature importances.\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Convert the importances into a DataFrame.\n",
        "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
        "\n",
        "# Sort the DataFrame by importance.\n",
        "feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
        "\n",
        "# Print the feature importances.\n",
        "print(feature_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc5f209",
      "metadata": {
        "id": "0cc5f209"
      },
      "outputs": [],
      "source": [
        "# Visualize the feature importances.\n",
        "plt.figure(figsize=(12, 4))\n",
        "bar_plot = sns.barplot(x='importance', y='feature', data=feature_importances)\n",
        "\n",
        "# Add the values on the bars.\n",
        "for i in range(feature_importances.shape[0]):\n",
        "    bar_plot.text(x=feature_importances.importance.iloc[i],\n",
        "                  y=i,\n",
        "                  s='{:.2f}'.format(feature_importances.importance.iloc[i]),\n",
        "                  va='center')\n",
        "\n",
        "# Add labels and title.\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot.\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}